# Home_Sales Big Data Challenge
## Assignment Deliverables
- [x] Create a Spark data frame and temporary table
- [x] Query the data to answer four questions
- [x] Cache the table and re-run the last query
- [x] Create a partition of the dataset, format it as a parquet file, convert it to a temporary table, and re-run the last query
- [x] Uncache the initial temporary table and verify

## Analysis
- Query: ![image](https://github.com/lvit001/Home_Sales_Big_Data/assets/140283164/c89a1c8a-baf2-4b74-8144-70e6b0c00a2f)
- Initial Data vs Cached Data: ![image](https://github.com/lvit001/Home_Sales_Big_Data/assets/140283164/29227fb1-2594-4f47-ade5-aff2480f38f6)
- Initial Data vs Partitioned Parquet Data: ![image](https://github.com/lvit001/Home_Sales_Big_Data/assets/140283164/05b276f4-7e62-4923-be0a-79b12e1dcec0)
- Secondary Query: ![image](https://github.com/lvit001/Home_Sales_Big_Data/assets/140283164/ccc67661-4ba4-4c38-b47c-dd24e560dbb0)
- Initial Data vs Partitioned Parquet Data on Secondary Query: ![image](https://github.com/lvit001/Home_Sales_Big_Data/assets/140283164/ce9d4c1c-9c8b-43ff-9331-44782f41de84)
